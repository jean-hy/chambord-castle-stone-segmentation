{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the folders to clean\n",
    "folders_to_clean = [\n",
    "    \"../data/patches/images\",  \n",
    "    \"../data/patches/masks\" \n",
    "]\n",
    "\n",
    "# Iterate over each folder\n",
    "for folder_path in folders_to_clean:\n",
    "    # Check if the folder exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "        continue\n",
    "\n",
    "    # Iterate over all files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        # Check if the file has a .jpg extension (case-insensitive)\n",
    "        if file_name.lower().endswith(\".png\"):\n",
    "            # Create the full file path\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            # Remove the file\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted: {file_path}\")\n",
    "\n",
    "print(\"All .jpg files have been deleted from specified folders.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the patches from the refined folder into 256x256 patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import sys\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 1) Environment Setup\n",
    "# -------------------------------------------------------------\n",
    "# Add the path to the Segment Anything library\n",
    "sys.path.append(\"../third_party/segment-anything/\")\n",
    "\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 2) Load or Initialize Annotations from JSON\n",
    "# -------------------------------------------------------------\n",
    "annotations_path = \"../data/annotations/boxes.json\"\n",
    "if os.path.exists(annotations_path):\n",
    "    with open(annotations_path, \"r\") as f:\n",
    "        boxes_dict = json.load(f)\n",
    "else:\n",
    "    boxes_dict = {}  # Start with an empty dictionary if no file exists\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 3) Define build_totalmask() function\n",
    "# -------------------------------------------------------------\n",
    "def build_totalmask(pred) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Builds a binary mask from SAM predictions.\n",
    "    Stones = white (255), mortar = black (0).\n",
    "    Fills small holes using morphological closing.\n",
    "    \n",
    "    Input:\n",
    "      - pred: list of dicts (output of SAM segmentation)\n",
    "    Output:\n",
    "      - Binary mask (np.uint8) or None if no masks generated.\n",
    "    \"\"\"\n",
    "    if len(pred) == 0:\n",
    "        return None\n",
    "\n",
    "    height, width = pred[0]['segmentation'].shape\n",
    "    total_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    for seg in pred:\n",
    "        seg_bin = seg['segmentation'].astype(np.uint8)\n",
    "        total_mask += seg_bin\n",
    "\n",
    "    _, total_mask_bin = cv2.threshold(total_mask, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    total_mask_bin = cv2.morphologyEx(total_mask_bin, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    return total_mask_bin\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 4) Load the SAM Model and Create Generators/Predictors\n",
    "# -------------------------------------------------------------\n",
    "sam_checkpoint = \"../models/sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "device = \"cpu\"  # Change to \"cuda\" if you have a GPU\n",
    "\n",
    "print(\"Loading SAM model on device:\", device)\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "mask_generator = SamAutomaticMaskGenerator(\n",
    "    model=sam,\n",
    "    points_per_side=48,\n",
    "    pred_iou_thresh=0.9,\n",
    "    stability_score_thresh=0.95,\n",
    "    crop_n_layers=1,\n",
    "    crop_n_points_downscale_factor=2,\n",
    "    min_mask_region_area=80\n",
    ")\n",
    "\n",
    "mask_predictor = SamPredictor(sam)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 5) Process Images in the Refined Folder and Extract Patches\n",
    "# -------------------------------------------------------------\n",
    "refined_folder = \"../data/refined/img-stones\"  # Adjust as needed\n",
    "output_dir_img = \"../data/patches/images\"\n",
    "output_dir_mask = \"../data/patches/masks\"\n",
    "os.makedirs(output_dir_img, exist_ok=True)\n",
    "os.makedirs(output_dir_mask, exist_ok=True)\n",
    "\n",
    "patch_size = 256\n",
    "stride = 128\n",
    "stone_threshold = 0.1  # 10% of the patch must be stone\n",
    "\n",
    "# Gather image files\n",
    "image_files = [\n",
    "    f for f in os.listdir(refined_folder)\n",
    "    if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "]\n",
    "\n",
    "patch_count = 0\n",
    "\n",
    "for img_file in image_files:\n",
    "    img_path = os.path.join(refined_folder, img_file)\n",
    "    image_bgr = cv2.imread(img_path)\n",
    "    if image_bgr is None:\n",
    "        print(f\"Skipping {img_file} -> Unable to load image.\")\n",
    "        continue\n",
    "\n",
    "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "    height, width, _ = image_rgb.shape\n",
    "    print(f\"\\n--- Processing {img_file} ---\")\n",
    "    print(f\"Image path: {img_path}\")\n",
    "    print(f\"Image size: {width} x {height}\")\n",
    "\n",
    "    # 5A) Generate Automatic Masks with SAM\n",
    "    masks_auto = mask_generator.generate(image_rgb)\n",
    "    print(f\"  Number of auto masks: {len(masks_auto)}\")\n",
    "    if len(masks_auto) == 0:\n",
    "        print(\"  No auto masks found; skipping.\")\n",
    "        continue\n",
    "\n",
    "    auto_mask_bin = build_totalmask(masks_auto)\n",
    "    if auto_mask_bin is None:\n",
    "        print(\"  auto_mask_bin is None; skipping.\")\n",
    "        continue\n",
    "\n",
    "    # 5B) Display the image and its auto mask\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image_rgb)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(auto_mask_bin, cmap=\"gray\")\n",
    "    plt.title(\"Automatic Mask\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    # 5C) Check if annotation for missing stones exists in the JSON.\n",
    "    # If not, ask the user whether to annotate.\n",
    "    if img_file not in boxes_dict:\n",
    "        user_response = input(f\"Annotation for {img_file} not found. Do you want to annotate missing stones? (y/n): \")\n",
    "        if user_response.lower().startswith(\"y\"):\n",
    "            # Call create-boxes.py with the image path.\n",
    "            # It should update (or write) the boxes into the same JSON file.\n",
    "            subprocess.run([\"python\", \"create-boxes.py\", img_path])\n",
    "            # Reload the JSON file to update boxes_dict.\n",
    "            if os.path.exists(annotations_path):\n",
    "                with open(annotations_path, \"r\") as f:\n",
    "                    boxes_dict = json.load(f)\n",
    "            else:\n",
    "                print(\"Annotation file not updated; proceeding without manual boxes.\")\n",
    "\n",
    "    # 5D) If the JSON now has boxes for this image, refine the mask.\n",
    "    if img_file in boxes_dict:\n",
    "        manual_boxes = boxes_dict[img_file]\n",
    "        if len(manual_boxes) > 0:\n",
    "            mask_predictor.set_image(image_rgb)\n",
    "            input_boxes = torch.tensor(manual_boxes, device=mask_predictor.device)\n",
    "            transformed_boxes = mask_predictor.transform.apply_boxes_torch(\n",
    "                input_boxes, image_rgb.shape[:2]\n",
    "            )\n",
    "            masks_box, scores, logits = mask_predictor.predict_torch(\n",
    "                point_coords=None,\n",
    "                point_labels=None,\n",
    "                boxes=transformed_boxes,\n",
    "                multimask_output=False\n",
    "            )\n",
    "            masks_box = masks_box.squeeze(1).cpu().numpy()  # (num_boxes, H, W)\n",
    "            combined_mask_bool = auto_mask_bin.astype(bool)\n",
    "            for i in range(masks_box.shape[0]):\n",
    "                stone_bool = masks_box[i].astype(bool)\n",
    "                combined_mask_bool = np.logical_or(combined_mask_bool, stone_bool)\n",
    "            final_mask_bin = (combined_mask_bool.astype(np.uint8) * 255)\n",
    "        else:\n",
    "            final_mask_bin = auto_mask_bin\n",
    "    else:\n",
    "        final_mask_bin = auto_mask_bin\n",
    "\n",
    "    # 5E) Check that the final mask contains stone areas.\n",
    "    nz_count = cv2.countNonZero(final_mask_bin)\n",
    "    print(f\"  Non-zero pixels in final mask: {nz_count}\")\n",
    "    if nz_count == 0:\n",
    "        print(\"  Final mask is all black; skipping.\")\n",
    "        continue\n",
    "\n",
    "    # 5F) Extract 256x256 patches from the final mask and original image.\n",
    "    if height < patch_size or width < patch_size:\n",
    "        print(f\"  {img_file} is too small for {patch_size}x{patch_size} patches; skipping.\")\n",
    "        continue\n",
    "\n",
    "    local_patch_count = 0\n",
    "    for y in range(0, height - patch_size + 1, stride):\n",
    "        for x in range(0, width - patch_size + 1, stride):\n",
    "            patch_mask = final_mask_bin[y:y+patch_size, x:x+patch_size]\n",
    "            patch_img = image_rgb[y:y+patch_size, x:x+patch_size]\n",
    "\n",
    "            stone_pixels = cv2.countNonZero(patch_mask)\n",
    "            total_pixels = patch_size * patch_size\n",
    "            if stone_pixels > stone_threshold * total_pixels:\n",
    "                patch_img_bgr = cv2.cvtColor(patch_img, cv2.COLOR_RGB2BGR)\n",
    "                patch_img_name = f\"patch_img_{patch_count}.png\"\n",
    "                patch_mask_name = f\"patch_mask_{patch_count}.png\"\n",
    "                cv2.imwrite(os.path.join(output_dir_img, patch_img_name), patch_img_bgr)\n",
    "                cv2.imwrite(os.path.join(output_dir_mask, patch_mask_name), patch_mask)\n",
    "                patch_count += 1\n",
    "                local_patch_count += 1\n",
    "\n",
    "    print(f\"  -> Patches saved from {img_file}: {local_patch_count}\")\n",
    "\n",
    "print(\"\\nDone!\")\n",
    "print(\"Total patches saved:\", patch_count)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
