{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the folders to clean\n",
    "folders_to_clean = [\n",
    "    \"../data/patches/images\",  \n",
    "    \"../data/patches/masks\" \n",
    "]\n",
    "\n",
    "# Iterate over each folder\n",
    "for folder_path in folders_to_clean:\n",
    "    # Check if the folder exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "        continue\n",
    "\n",
    "    # Iterate over all files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        # Check if the file has a .jpg extension (case-insensitive)\n",
    "        if file_name.lower().endswith(\".png\"):\n",
    "            # Create the full file path\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            # Remove the file\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted: {file_path}\")\n",
    "\n",
    "print(\"All .jpg files have been deleted from specified folders.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the patches from the refined folder into 256x256 patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 1) Environment Setup\n",
    "# -------------------------------------------------------------\n",
    "# Add the path to the Segment Anything library\n",
    "sys.path.append(\"../third_party/segment-anything/\")\n",
    "\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 2) Define build_totalmask() function\n",
    "# -------------------------------------------------------------\n",
    "def build_totalmask(pred) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    input : list of dict, type: segment-anything\n",
    "    output : binary mask, type: np.uint8\n",
    "    \n",
    "    description :\n",
    "    - This function takes the output of the SAM model and builds a single binary mask.\n",
    "    - It uses Otsu thresholding to convert the mask into binary format.\n",
    "    - It also applies morphological closing to fill small holes in the mask.\n",
    "    - The function returns the binary mask.\n",
    "    - If no masks are generated, it returns None.\n",
    "\n",
    "    Builds a binary mask from SAM predictions.\n",
    "    Stones = white (255), mortar = black (0).\n",
    "    Fills small holes using morphological closing.\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    if len(pred) == 0:\n",
    "        # If no masks were generated, return None\n",
    "        return None\n",
    "\n",
    "    # Get image dimensions from the first mask\n",
    "    height, width = pred[0]['segmentation'].shape\n",
    "    total_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    # Accumulate all masks using summation\n",
    "    for seg in pred:\n",
    "        seg_bin = seg['segmentation'].astype(np.uint8)\n",
    "        total_mask += seg_bin\n",
    "\n",
    "    # Use Otsu thresholding to convert the summation into a binary mask\n",
    "    _, total_mask_bin = cv2.threshold(total_mask, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "\n",
    "    # Apply morphological closing to fill small holes\n",
    "    kernel = np.ones((2,2), np.uint8)\n",
    "    total_mask_bin = cv2.morphologyEx(total_mask_bin, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    return total_mask_bin\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 3) Load the SAM model\n",
    "# -------------------------------------------------------------\n",
    "sam_checkpoint = \"../models/sam_vit_h_4b8939.pth\" \n",
    "model_type = \"vit_h\"\n",
    "device = \"cpu\"  # or \"cuda\" if we want to use a GPU\n",
    "\n",
    "print(\"Loading SAM model on device:\", device)\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "# Create the automatic mask generator (for the initial segmentation)\n",
    "mask_generator = SamAutomaticMaskGenerator(\n",
    "    model=sam,\n",
    "    points_per_side=32,\n",
    "    pred_iou_thresh=0.86,\n",
    "    stability_score_thresh=0.92,\n",
    "    crop_n_layers=1,\n",
    "    crop_n_points_downscale_factor=2\n",
    ")\n",
    "\n",
    "# Create a SamPredictor for bounding-box refinement\n",
    "mask_predictor = SamPredictor(sam)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 4) Define bounding boxes for each image :\n",
    "#   \n",
    "# -------------------------------------------------------------\n",
    "boxes_dict = {\n",
    "    # \"filename.jpg\": [ (x0, y0, x1, y1), (x0, y0, x1, y1), ... ],\n",
    "    \"FSE_35_004.jpg\": [\n",
    "        (4, 12, 178, 107), (182, 12, 382, 94), (92, 111, 298, 203), (220, 303, 409, 405), (414, 302, 492, 395), (498, 303, 561, 394), (520, 199, 561, 301), (3, 319, 23, 414)\n",
    "    ],\n",
    "    # We repeat for other images:\n",
    "    # \"FSE_24_010.jpg\": [...],\n",
    "    # \"FNE_100_136.jpg\": [...],\n",
    "    # etc.\n",
    "}\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 5) Loop over images in the refined folder\n",
    "#    Then refine segmentation with bounding boxes\n",
    "# -------------------------------------------------------------\n",
    "refined_folder = \"../data/refined/img-stones\"  # Adjust path if needed\n",
    "output_dir_img = \"../data/patches/images\"\n",
    "output_dir_mask = \"../data/patches/masks\"\n",
    "os.makedirs(output_dir_img, exist_ok=True)\n",
    "os.makedirs(output_dir_mask, exist_ok=True)\n",
    "\n",
    "patch_size = 256\n",
    "stride = 128\n",
    "stone_threshold = 0.1  # 10% of the patch must be stone\n",
    "\n",
    "# Gather image files\n",
    "image_files = [\n",
    "    f for f in os.listdir(refined_folder)\n",
    "    if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "]\n",
    "\n",
    "patch_count = 0\n",
    "\n",
    "for img_file in image_files:\n",
    "    img_path = os.path.join(refined_folder, img_file)\n",
    "    image_bgr = cv2.imread(img_path)\n",
    "\n",
    "    if image_bgr is None:\n",
    "        print(f\"Skipping {img_file} -> Unable to load image.\")\n",
    "        continue\n",
    "\n",
    "    # Convert to RGB\n",
    "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "    height, width, _ = image_rgb.shape\n",
    "    print(f\"\\n--- Processing {img_file} ---\")\n",
    "    print(f\"Image path: {img_path}\")\n",
    "    print(f\"Image size: {width} x {height}\")\n",
    "\n",
    "    # 5A) Generate Automatic Masks with SAM\n",
    "    masks_auto = mask_generator.generate(image_rgb)\n",
    "    print(f\"  Number of auto masks: {len(masks_auto)}\")\n",
    "\n",
    "    if len(masks_auto) == 0:\n",
    "        print(\"  No auto masks found; skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Build the initial mask from auto\n",
    "    auto_mask_bin = build_totalmask(masks_auto)\n",
    "    if auto_mask_bin is None:\n",
    "        print(\"  auto_mask_bin is None; skipping.\")\n",
    "        continue\n",
    "\n",
    "    # 5B) Check if we have bounding boxes for this image\n",
    "    if img_file in boxes_dict:\n",
    "        # 1) Set the image on the predictor\n",
    "        mask_predictor.set_image(image_rgb)\n",
    "\n",
    "        # 2) Convert bounding boxes to Torch tensor\n",
    "        raw_boxes = boxes_dict[img_file]\n",
    "        input_boxes = torch.tensor(raw_boxes, device=mask_predictor.device)\n",
    "\n",
    "        # 3) Transform boxes for SAM\n",
    "        transformed_boxes = mask_predictor.transform.apply_boxes_torch(\n",
    "            input_boxes, \n",
    "            image_rgb.shape[:2]\n",
    "        )\n",
    "\n",
    "        # 4) Predict masks for each bounding box\n",
    "        masks_box, scores, logits = mask_predictor.predict_torch(\n",
    "            point_coords=None,\n",
    "            point_labels=None,\n",
    "            boxes=transformed_boxes,\n",
    "            multimask_output=False\n",
    "        )\n",
    "        # shape: (num_boxes, 1, H, W)\n",
    "        masks_box = masks_box.squeeze(1).cpu().numpy()  # -> (num_boxes, H, W)\n",
    "\n",
    "        # 5) Merge bounding-box masks with the auto mask\n",
    "        combined_mask_bool = auto_mask_bin.astype(bool)\n",
    "        for i in range(masks_box.shape[0]):\n",
    "            # OR the bounding-box mask into the auto mask\n",
    "            stone_bool = masks_box[i].astype(bool)\n",
    "            combined_mask_bool = np.logical_or(combined_mask_bool, stone_bool)\n",
    "\n",
    "        # Convert back to 0/255\n",
    "        final_mask_bin = (combined_mask_bool.astype(np.uint8) * 255)\n",
    "    else:\n",
    "        # If no bounding boxes for this image, just use the auto mask\n",
    "        final_mask_bin = auto_mask_bin\n",
    "\n",
    "    # 5C) Check how many nonzero pixels in the final mask\n",
    "    nz_count = cv2.countNonZero(final_mask_bin)\n",
    "    print(f\"  Non-zero pixels in final_mask_bin: {nz_count}\")\n",
    "\n",
    "    if nz_count == 0:\n",
    "        print(\"  final_mask_bin is all black; skipping.\")\n",
    "        continue\n",
    "\n",
    "    # 5D) Slide over the image to extract patches\n",
    "    if height < patch_size or width < patch_size:\n",
    "        print(f\"  {img_file} is too small for {patch_size}x{patch_size} patches; skipping.\")\n",
    "        continue\n",
    "\n",
    "    local_patch_count = 0\n",
    "    for y in range(0, height - patch_size + 1, stride):\n",
    "        for x in range(0, width - patch_size + 1, stride):\n",
    "            patch_mask = final_mask_bin[y:y+patch_size, x:x+patch_size]\n",
    "            patch_img = image_rgb[y:y+patch_size, x:x+patch_size]\n",
    "\n",
    "            stone_pixels = cv2.countNonZero(patch_mask)\n",
    "            total_pixels = patch_size * patch_size\n",
    "\n",
    "            # If patch meets the stone threshold\n",
    "            if stone_pixels > stone_threshold * total_pixels:\n",
    "                patch_img_bgr = cv2.cvtColor(patch_img, cv2.COLOR_RGB2BGR)\n",
    "                patch_img_name = f\"patch_img_{patch_count}.png\"\n",
    "                patch_mask_name = f\"patch_mask_{patch_count}.png\"\n",
    "                cv2.imwrite(os.path.join(output_dir_img, patch_img_name), patch_img_bgr)\n",
    "                cv2.imwrite(os.path.join(output_dir_mask, patch_mask_name), patch_mask)\n",
    "                \n",
    "                patch_count += 1\n",
    "                local_patch_count += 1\n",
    "\n",
    "    print(f\"  -> Patches saved from {img_file}: {local_patch_count}\")\n",
    "\n",
    "print(\"\\nDone!\")\n",
    "print(\"Total patches saved:\", patch_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
