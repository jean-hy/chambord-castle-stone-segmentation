{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-Tuning SegFormer (PyTorch) for Stone vs. Separation\n",
        "\n",
        "This notebook fine-tunes a SegFormer model (e.g., `nvidia/mit-b4`) from the Hugging Face Hub for the task of segmenting stone vs. separation regions.\n",
        "\n",
        "**Dataset Structure:**\n",
        "- **Images:** `../data/augmented/images`\n",
        "- **Masks:**  `../data/augmented/masks`\n",
        "\n",
        "**Details:**\n",
        "- Input images are resized to **256 x 256** pixels.\n",
        "- The model outputs logits that are upsampled to the original image size for visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset, Image\n",
        "from transformers import (\n",
        "    SegformerForSemanticSegmentation, \n",
        "    SegformerImageProcessor, \n",
        "    TrainingArguments, \n",
        "    Trainer\n",
        ")\n",
        "import evaluate\n",
        "\n",
        "# Set default plot size for visualization\n",
        "plt.rcParams['figure.figsize'] = (12, 12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Settings and Configurations\n",
        "\n",
        "Define dataset directories, choose the pre-trained model checkpoint, and set training parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset directories for images and masks\n",
        "IMAGE_DIR = \"../data/augmented/images\"\n",
        "MASK_DIR  = \"../data/augmented/masks\"\n",
        "\n",
        "# Select pre-trained model checkpoint from Hugging Face Hub.\n",
        "# Options: \"nvidia/mit-b0\", \"nvidia/mit-b1\", \"nvidia/mit-b2\", \"nvidia/mit-b3\", \"nvidia/mit-b4\", \"nvidia/mit-b5\"\n",
        "MODEL_CHECKPOINT = \"nvidia/mit-b4\"\n",
        "\n",
        "# Training parameters\n",
        "VAL_SIZE = 0.1       # 10% of the data will be used for validation\n",
        "BATCH_SIZE = 2\n",
        "EPOCHS = 1\n",
        "LR = 6e-5            # Learning rate\n",
        "IMG_SIZE = 256       # Resize images to 256 x 256 pixels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preparation\n",
        "\n",
        "We load image and mask file paths, perform a train/validation split, and convert them into Hugging Face datasets.\n",
        "The images are cast to the `Image()` type so they are automatically loaded during transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List image and mask filenames (ignoring hidden files)\n",
        "image_files = sorted([f for f in os.listdir(IMAGE_DIR) if not f.startswith('.')])\n",
        "mask_files  = sorted([f for f in os.listdir(MASK_DIR) if not f.startswith('.')])\n",
        "\n",
        "# Build full paths for images and masks\n",
        "images = [os.path.join(IMAGE_DIR, f) for f in image_files]\n",
        "masks  = [os.path.join(MASK_DIR, f) for f in mask_files]\n",
        "\n",
        "print(f\"Total images found: {len(images)}\")\n",
        "print(f\"Total masks found:  {len(masks)}\")\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_images, val_images, train_masks, val_masks = train_test_split(\n",
        "    images, masks, test_size=VAL_SIZE, random_state=42, shuffle=True\n",
        ")\n",
        "print(f\"Training images: {len(train_images)}\")\n",
        "print(f\"Validation images: {len(val_images)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_dataset(image_paths, mask_paths):\n",
        "    \"\"\"Create a Hugging Face dataset from image and mask file paths.\"\"\"\n",
        "    dataset = Dataset.from_dict({\n",
        "        'pixel_values': image_paths,\n",
        "        'label': mask_paths\n",
        "    })\n",
        "    # Cast the columns to Image() to enable auto-loading\n",
        "    dataset = dataset.cast_column('pixel_values', Image())\n",
        "    dataset = dataset.cast_column('label', Image())\n",
        "    return dataset\n",
        "\n",
        "# Create datasets for training and validation\n",
        "ds_train = create_dataset(train_images, train_masks)\n",
        "ds_valid = create_dataset(val_images, val_masks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Transformation\n",
        "\n",
        "We initialize the `SegformerImageProcessor` to handle resizing, normalization, and label encoding.\n",
        "The `apply_transforms` function processes each batch of images and masks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the feature extractor from the chosen model checkpoint\n",
        "feature_extractor = SegformerImageProcessor.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "def apply_transforms(batch):\n",
        "    \"\"\"\n",
        "    Apply preprocessing to a batch of images and masks:\n",
        "      - Resizes images to (IMG_SIZE, IMG_SIZE)\n",
        "      - Normalizes images and processes labels\n",
        "    \"\"\"\n",
        "    images = batch['pixel_values']\n",
        "    labels = batch['label']\n",
        "    # Process images and labels: resize, normalize, etc.\n",
        "    inputs = feature_extractor(images, labels, size=(IMG_SIZE, IMG_SIZE), return_tensors=\"pt\")\n",
        "    inputs[\"pixel_values\"] = inputs[\"pixel_values\"].contiguous()\n",
        "    inputs[\"labels\"] = inputs[\"labels\"].contiguous()\n",
        "    return inputs\n",
        "\n",
        "# Set the transformation for the datasets (batch-level processing)\n",
        "ds_train.set_transform(apply_transforms)\n",
        "ds_valid.set_transform(apply_transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Setup\n",
        "\n",
        "We define the SegFormer model for our segmentation task. In this case:\n",
        "- **Class 0:** Stone\n",
        "- **Class 1:** Separation\n",
        "\n",
        "Label mappings are specified via `id2label` and `label2id`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b4 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Define label mappings\n",
        "id2label = {0: \"stone\", 1: \"separation\"}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "num_labels = len(id2label)\n",
        "\n",
        "# Load the pre-trained SegFormer model and modify the head for our segmentation task\n",
        "model = SegformerForSemanticSegmentation.from_pretrained(\n",
        "    MODEL_CHECKPOINT,\n",
        "    num_labels=num_labels,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    ignore_mismatched_sizes=True  # Allows for different output head dimensions\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Define Evaluation Metrics\n",
        "\n",
        "We use the Mean Intersection over Union (Mean IoU) metric to evaluate performance.\n",
        "The logits output by the model are upsampled to the ground truth mask size before computing the metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the Mean IoU metric\n",
        "metric = evaluate.load('mean_iou')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    with torch.no_grad():\n",
        "        logits, labels = eval_pred\n",
        "        # Ensure the NumPy array is contiguous\n",
        "        logits_np = np.ascontiguousarray(logits)\n",
        "        # Convert to a torch tensor and force a reshape to match the original shape\n",
        "        logits_tensor = torch.from_numpy(logits_np).reshape(logits.shape)\n",
        "        print(f\"Logits shape: {logits_tensor.shape}\")\n",
        "        print(\"Strides:\", logits_tensor.stride())\n",
        "\n",
        "        # Upsample the logits to the size of the ground-truth mask\n",
        "        logits_tensor = nn.functional.interpolate(\n",
        "            logits_tensor,\n",
        "            size=labels.shape[-2:],\n",
        "            mode='bilinear',\n",
        "            align_corners=False,\n",
        "        )\n",
        "        # Get predicted class labels\n",
        "        pred_labels = logits_tensor.argmax(dim=1).detach().cpu().numpy()\n",
        "        \n",
        "        # Compute metrics using the Mean IoU metric\n",
        "        results = metric._compute(\n",
        "            predictions=pred_labels,\n",
        "            references=labels,\n",
        "            num_labels=num_labels,\n",
        "            ignore_index=0,  \n",
        "            reduce_labels=feature_extractor.do_reduce_labels,\n",
        "        )\n",
        "        \n",
        "        # Include per-category metrics in the results\n",
        "        per_category_accuracy = results.pop(\"per_category_accuracy\").tolist()\n",
        "        per_category_iou = results.pop(\"per_category_iou\").tolist()\n",
        "        results.update({f\"accuracy_{id2label[i]}\": v for i, v in enumerate(per_category_accuracy)})\n",
        "        results.update({f\"iou_{id2label[i]}\": v for i, v in enumerate(per_category_iou)})\n",
        "        \n",
        "        return results\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training Setup\n",
        "\n",
        "We configure the training using Hugging Face's `TrainingArguments` and initialize the `Trainer`.  \n",
        "Evaluation and checkpointing occur every 20 steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 28\u001b[0m\n\u001b[1;32m     19\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     20\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     21\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Save the fine-tuned model to disk\u001b[39;00m\n\u001b[1;32m     31\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegformer_stone\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/Desktop/chambord-stone-segmentation/venv/lib/python3.13/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Desktop/chambord-stone-segmentation/venv/lib/python3.13/site-packages/transformers/trainer.py:2548\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2541\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2542\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2544\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2546\u001b[0m )\n\u001b[1;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2554\u001b[0m ):\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
            "File \u001b[0;32m~/Desktop/chambord-stone-segmentation/venv/lib/python3.13/site-packages/transformers/trainer.py:3740\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   3738\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 3740\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3742\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
            "File \u001b[0;32m~/Desktop/chambord-stone-segmentation/venv/lib/python3.13/site-packages/accelerate/accelerator.py:2329\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2328\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2329\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Desktop/chambord-stone-segmentation/venv/lib/python3.13/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Desktop/chambord-stone-segmentation/venv/lib/python3.13/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Desktop/chambord-stone-segmentation/venv/lib/python3.13/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"segformer_stone_finetuned\",\n",
        "    learning_rate=LR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=20,\n",
        "    eval_steps=20,\n",
        "    logging_steps=1,\n",
        "    save_total_limit=3,\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=False,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Initialize the Trainer with the model, datasets, and metric computation function\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=ds_train,\n",
        "    eval_dataset=ds_valid,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model to disk\n",
        "model.save_pretrained(\"segformer_stone\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Inference on the Validation Set\n",
        "\n",
        "We run inference on the validation set and display:\n",
        "- The original image\n",
        "- The ground truth mask\n",
        "- The predicted mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(len(val_images)):\n",
        "    image_path = val_images[i]\n",
        "    mask_path = val_masks[i]\n",
        "    \n",
        "    # Load image and mask using OpenCV\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
        "    mask = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
        "    print(f\"Validation image #{i + 1}\")\n",
        "    \n",
        "    # Prepare input for the model using the feature extractor\n",
        "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    logits = torch.tensor(outputs.logits.detach().cpu().numpy(), device=outputs.logits.device)\n",
        "\n",
        "    \n",
        "    # Upsample logits to the original image size\n",
        "    upsampled_logits = nn.functional.interpolate(\n",
        "        logits,\n",
        "        size=image.shape[:2],\n",
        "        mode=\"bilinear\",\n",
        "        align_corners=False\n",
        "    )\n",
        "    pred_mask = upsampled_logits.argmax(dim=1)[0].cpu().numpy()\n",
        "    \n",
        "    # Plot the original image, ground truth mask, and predicted mask side-by-side\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "    axes[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    axes[0].set_title(\"Original Image\")\n",
        "    axes[0].axis(\"off\")\n",
        "    \n",
        "    axes[1].imshow(mask, cmap=\"gray\")\n",
        "    axes[1].set_title(\"Ground Truth Mask\")\n",
        "    axes[1].axis(\"off\")\n",
        "    \n",
        "    axes[2].imshow(pred_mask, cmap=\"gray\")\n",
        "    axes[2].set_title(\"Predicted Mask\")\n",
        "    axes[2].axis(\"off\")\n",
        "    \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Inference on the Test Set\n",
        "\n",
        "If a test set exists, this section processes each test image and saves the predicted masks to disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TEST_DIR = \"../data/augmented/test_images\"\n",
        "if os.path.exists(TEST_DIR):\n",
        "    test_images = sorted([os.path.join(TEST_DIR, f) for f in os.listdir(TEST_DIR) if not f.startswith('.')])\n",
        "    output_dir = \"test_predictions\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    for img_path in test_images:\n",
        "        image = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
        "        inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        \n",
        "        # Upsample logits to original image dimensions\n",
        "        upsampled_logits = nn.functional.interpolate(\n",
        "            logits,\n",
        "            size=image.shape[:2],\n",
        "            mode=\"bilinear\",\n",
        "            align_corners=False\n",
        "        )\n",
        "        pred_mask = upsampled_logits.argmax(dim=1)[0].cpu().numpy()\n",
        "        \n",
        "        # Save the predicted mask using the original image filename\n",
        "        filename = os.path.basename(img_path)\n",
        "        save_path = os.path.join(output_dir, f\"mask_{filename}\")\n",
        "        plt.imsave(save_path, pred_mask, cmap=\"gray\")\n",
        "    \n",
        "    print(f\"Predicted masks saved to {output_dir}/\")\n",
        "else:\n",
        "    print(\"Test directory not found. Skipping test inference.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
