{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-Tuning SegFormer (PyTorch) for Stone vs. Separation\n",
        "\n",
        "This notebook fine-tunes a SegFormer model (e.g., `nvidia/mit-b4`) from the Hugging Face Hub for the task of segmenting stone vs. separation regions.\n",
        "\n",
        "**Dataset Structure:**\n",
        "- **Images:** `../data/augmented/images`\n",
        "- **Masks:**  `../data/augmented/masks`\n",
        "\n",
        "**Details:**\n",
        "- Input images are resized to **256 x 256** pixels.\n",
        "- The model outputs logits that are upsampled to the original image size for visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset, Image\n",
        "from transformers import (\n",
        "    SegformerForSemanticSegmentation, \n",
        "    SegformerImageProcessor, \n",
        "    TrainingArguments, \n",
        "    Trainer\n",
        ")\n",
        "import evaluate\n",
        "\n",
        "# Set default plot size for visualization\n",
        "plt.rcParams['figure.figsize'] = (12, 12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Settings and Configurations\n",
        "\n",
        "Define dataset directories, choose the pre-trained model checkpoint, and set training parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset directories for images and masks\n",
        "IMAGE_DIR = \"../data/augmented/images\"\n",
        "MASK_DIR  = \"../data/augmented/masks\"\n",
        "\n",
        "# Select pre-trained model checkpoint from Hugging Face Hub.\n",
        "# Options: \"nvidia/mit-b0\", \"nvidia/mit-b1\", \"nvidia/mit-b2\", \"nvidia/mit-b3\", \"nvidia/mit-b4\", \"nvidia/mit-b5\"\n",
        "MODEL_CHECKPOINT = \"nvidia/mit-b4\"\n",
        "\n",
        "# Training parameters\n",
        "VAL_SIZE = 0.1       # 10% of the data will be used for validation\n",
        "BATCH_SIZE = 2\n",
        "EPOCHS = 1\n",
        "LR = 6e-5            # Learning rate\n",
        "IMG_SIZE = 256       # Resize images to 256 x 256 pixels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preparation\n",
        "\n",
        "We load image and mask file paths, perform a train/validation split, and convert them into Hugging Face datasets.\n",
        "The images are cast to the `Image()` type so they are automatically loaded during transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List image and mask filenames (ignoring hidden files)\n",
        "image_files = sorted([f for f in os.listdir(IMAGE_DIR) if not f.startswith('.')])\n",
        "mask_files  = sorted([f for f in os.listdir(MASK_DIR) if not f.startswith('.')])\n",
        "\n",
        "# Build full paths for images and masks\n",
        "images = [os.path.join(IMAGE_DIR, f) for f in image_files]\n",
        "masks  = [os.path.join(MASK_DIR, f) for f in mask_files]\n",
        "\n",
        "print(f\"Total images found: {len(images)}\")\n",
        "print(f\"Total masks found:  {len(masks)}\")\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_images, val_images, train_masks, val_masks = train_test_split(\n",
        "    images, masks, test_size=VAL_SIZE, random_state=42, shuffle=True\n",
        ")\n",
        "print(f\"Training images: {len(train_images)}\")\n",
        "print(f\"Validation images: {len(val_images)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_dataset(image_paths, mask_paths):\n",
        "    \"\"\"Create a Hugging Face dataset from image and mask file paths.\"\"\"\n",
        "    dataset = Dataset.from_dict({\n",
        "        'pixel_values': image_paths,\n",
        "        'label': mask_paths\n",
        "    })\n",
        "    # Cast the columns to Image() to enable auto-loading\n",
        "    dataset = dataset.cast_column('pixel_values', Image())\n",
        "    dataset = dataset.cast_column('label', Image())\n",
        "    return dataset\n",
        "\n",
        "# Create datasets for training and validation\n",
        "ds_train = create_dataset(train_images, train_masks)\n",
        "ds_valid = create_dataset(val_images, val_masks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Transformation\n",
        "\n",
        "We initialize the `SegformerImageProcessor` to handle resizing, normalization, and label encoding.\n",
        "The `apply_transforms` function processes each batch of images and masks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the feature extractor from the chosen model checkpoint\n",
        "feature_extractor = SegformerImageProcessor.from_pretrained(MODEL_CHECKPOINT)\n",
        "\n",
        "def apply_transforms(batch):\n",
        "    \"\"\"\n",
        "    Apply preprocessing to a batch of images and masks:\n",
        "      - Resizes images to (IMG_SIZE, IMG_SIZE)\n",
        "      - Normalizes images and processes labels\n",
        "    \"\"\"\n",
        "    images = batch['pixel_values']\n",
        "    labels = batch['label']\n",
        "    # Process images and labels: resize, normalize, etc.\n",
        "    inputs = feature_extractor(images, labels, size=(IMG_SIZE, IMG_SIZE), return_tensors=\"pt\")\n",
        "    inputs[\"pixel_values\"] = inputs[\"pixel_values\"].contiguous()\n",
        "    inputs[\"labels\"] = inputs[\"labels\"].contiguous()\n",
        "    return inputs\n",
        "\n",
        "# Set the transformation for the datasets (batch-level processing)\n",
        "ds_train.set_transform(apply_transforms)\n",
        "ds_valid.set_transform(apply_transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Setup\n",
        "\n",
        "We define the SegFormer model for our segmentation task. In this case:\n",
        "- **Class 0:** Stone\n",
        "- **Class 1:** Separation\n",
        "\n",
        "Label mappings are specified via `id2label` and `label2id`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define label mappings\n",
        "id2label = {0: \"stone\", 1: \"separation\"}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "num_labels = len(id2label)\n",
        "\n",
        "# Load the pre-trained SegFormer model and modify the head for our segmentation task\n",
        "model = SegformerForSemanticSegmentation.from_pretrained(\n",
        "    MODEL_CHECKPOINT,\n",
        "    num_labels=num_labels,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    ignore_mismatched_sizes=True  # Allows for different output head dimensions\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Define Evaluation Metrics\n",
        "\n",
        "We use the Mean Intersection over Union (Mean IoU) metric to evaluate performance.\n",
        "The logits output by the model are upsampled to the ground truth mask size before computing the metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the Mean IoU metric\n",
        "metric = evaluate.load('mean_iou')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    with torch.no_grad():\n",
        "        logits, labels = eval_pred\n",
        "        # Ensure the NumPy array is contiguous\n",
        "        logits_np = np.ascontiguousarray(logits)\n",
        "        # Convert to a torch tensor and force a reshape to match the original shape\n",
        "        logits_tensor = torch.from_numpy(logits_np).reshape(logits.shape)\n",
        "        print(f\"Logits shape: {logits_tensor.shape}\")\n",
        "        print(\"Strides:\", logits_tensor.stride())\n",
        "\n",
        "        # Upsample the logits to the size of the ground-truth mask\n",
        "        logits_tensor = nn.functional.interpolate(\n",
        "            logits_tensor,\n",
        "            size=labels.shape[-2:],\n",
        "            mode='bilinear',\n",
        "            align_corners=False,\n",
        "        )\n",
        "        # Get predicted class labels\n",
        "        pred_labels = logits_tensor.argmax(dim=1).detach().cpu().numpy()\n",
        "        \n",
        "        # Compute metrics using the Mean IoU metric\n",
        "        results = metric._compute(\n",
        "            predictions=pred_labels,\n",
        "            references=labels,\n",
        "            num_labels=num_labels,\n",
        "            ignore_index=0,  \n",
        "            reduce_labels=feature_extractor.do_reduce_labels,\n",
        "        )\n",
        "        \n",
        "        # Include per-category metrics in the results\n",
        "        per_category_accuracy = results.pop(\"per_category_accuracy\").tolist()\n",
        "        per_category_iou = results.pop(\"per_category_iou\").tolist()\n",
        "        results.update({f\"accuracy_{id2label[i]}\": v for i, v in enumerate(per_category_accuracy)})\n",
        "        results.update({f\"iou_{id2label[i]}\": v for i, v in enumerate(per_category_iou)})\n",
        "        \n",
        "        return results\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training Setup\n",
        "\n",
        "We configure the training using Hugging Face's `TrainingArguments` and initialize the `Trainer`.  \n",
        "Evaluation and checkpointing occur every 20 steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"segformer_stone_finetuned\",\n",
        "    learning_rate=LR,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=20,\n",
        "    eval_steps=20,\n",
        "    logging_steps=1,\n",
        "    save_total_limit=3,\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=False,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Initialize the Trainer with the model, datasets, and metric computation function\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=ds_train,\n",
        "    eval_dataset=ds_valid,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model to disk\n",
        "model.save_pretrained(\"segformer_stone\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Inference on the Validation Set\n",
        "\n",
        "We run inference on the validation set and display:\n",
        "- The original image\n",
        "- The ground truth mask\n",
        "- The predicted mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(len(val_images)):\n",
        "    image_path = val_images[i]\n",
        "    mask_path = val_masks[i]\n",
        "    \n",
        "    # Load image and mask using OpenCV\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
        "    mask = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
        "    print(f\"Validation image #{i + 1}\")\n",
        "    \n",
        "    # Prepare input for the model using the feature extractor\n",
        "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    logits = torch.tensor(outputs.logits.detach().cpu().numpy(), device=outputs.logits.device)\n",
        "\n",
        "    \n",
        "    # Upsample logits to the original image size\n",
        "    upsampled_logits = nn.functional.interpolate(\n",
        "        logits,\n",
        "        size=image.shape[:2],\n",
        "        mode=\"bilinear\",\n",
        "        align_corners=False\n",
        "    )\n",
        "    pred_mask = upsampled_logits.argmax(dim=1)[0].cpu().numpy()\n",
        "    \n",
        "    # Plot the original image, ground truth mask, and predicted mask side-by-side\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "    axes[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    axes[0].set_title(\"Original Image\")\n",
        "    axes[0].axis(\"off\")\n",
        "    \n",
        "    axes[1].imshow(mask, cmap=\"gray\")\n",
        "    axes[1].set_title(\"Ground Truth Mask\")\n",
        "    axes[1].axis(\"off\")\n",
        "    \n",
        "    axes[2].imshow(pred_mask, cmap=\"gray\")\n",
        "    axes[2].set_title(\"Predicted Mask\")\n",
        "    axes[2].axis(\"off\")\n",
        "    \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Inference on the Test Set\n",
        "\n",
        "If a test set exists, this section processes each test image and saves the predicted masks to disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TEST_DIR = \"../data/augmented/test_images\"\n",
        "if os.path.exists(TEST_DIR):\n",
        "    test_images = sorted([os.path.join(TEST_DIR, f) for f in os.listdir(TEST_DIR) if not f.startswith('.')])\n",
        "    output_dir = \"test_predictions\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    for img_path in test_images:\n",
        "        image = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
        "        inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        \n",
        "        # Upsample logits to original image dimensions\n",
        "        upsampled_logits = nn.functional.interpolate(\n",
        "            logits,\n",
        "            size=image.shape[:2],\n",
        "            mode=\"bilinear\",\n",
        "            align_corners=False\n",
        "        )\n",
        "        pred_mask = upsampled_logits.argmax(dim=1)[0].cpu().numpy()\n",
        "        \n",
        "        # Save the predicted mask using the original image filename\n",
        "        filename = os.path.basename(img_path)\n",
        "        save_path = os.path.join(output_dir, f\"mask_{filename}\")\n",
        "        plt.imsave(save_path, pred_mask, cmap=\"gray\")\n",
        "    \n",
        "    print(f\"Predicted masks saved to {output_dir}/\")\n",
        "else:\n",
        "    print(\"Test directory not found. Skipping test inference.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
