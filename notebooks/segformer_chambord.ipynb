{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SegFormer for Stone Segmentation\n",
        "\n",
        "This notebook demonstrates how to implement and train a **SegFormer** model for stone segmentation. The workflow follows a similar structure to our previous notebooks and includes:\n",
        "\n",
        "1. Importing dependencies\n",
        "2. Loading and splitting the augmented dataset\n",
        "3. Creating a custom data generator\n",
        "4. Defining a simplified SegFormer model\n",
        "5. Compiling and training the model (or loading a saved model)\n",
        "6. Visualizing training curves and sample predictions\n",
        "7. Saving the trained model and predicted masks\n",
        "8. Evaluating model performance using IoU and Dice Coefficient metrics\n",
        "9. Concluding remarks\n",
        "\n",
        "The notebook is assumed to be stored as `segformer_chambord.ipynb` and uses the following dataset structure:\n",
        "  - Images: `../data/augmented/images`\n",
        "  - Masks:  `../data/augmented/masks`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install tensorflow\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Keras version:\", tf.keras.__version__)\n",
        "print(\"scikit-learn version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameters and Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define directories for images and masks\n",
        "IMAGES_DIR = \"../data/augmented/images\"\n",
        "MASKS_DIR = \"../data/augmented/masks\"\n",
        "\n",
        "# Image details\n",
        "IMAGE_HEIGHT = 256\n",
        "IMAGE_WIDTH = 256\n",
        "NUM_CLASSES = 2  # Binary segmentation: background and stones\n",
        "\n",
        "# Training parameters\n",
        "BATCH_SIZE = 4\n",
        "EPOCHS = 100\n",
        "VAL_SPLIT = 0.2  # 20% for validation\n",
        "\n",
        "# For reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Loading and Splitting the Dataset\n",
        "\n",
        "- The `images` folder contains the input images.\n",
        "- The `masks` folder contains corresponding masks (with matching filenames).\n",
        "\n",
        "We load the filenames, build full paths, and split the data into training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_image(image_path, target_size=(256, 256), grayscale=False):\n",
        "    \"\"\"\n",
        "    Loads an image from disk, resizes it, and converts it to an array.\n",
        "    \"\"\"\n",
        "    img = load_img(image_path, target_size=target_size, color_mode='grayscale' if grayscale else 'rgb')\n",
        "    return img_to_array(img)\n",
        "\n",
        "image_filenames = sorted(os.listdir(IMAGES_DIR))\n",
        "\n",
        "# Build full paths\n",
        "image_paths = [os.path.join(IMAGES_DIR, f) for f in image_filenames]\n",
        "mask_paths = [os.path.join(MASKS_DIR, f) for f in image_filenames]  # assume same filenames\n",
        "\n",
        "print(\"Total images found:\", len(image_paths))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train/Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_images, val_images, train_masks, val_masks = train_test_split(\n",
        "    image_paths, mask_paths, test_size=VAL_SPLIT, random_state=SEED\n",
        ")\n",
        "\n",
        "print(\"Training set size:\", len(train_images))\n",
        "print(\"Validation set size:\", len(val_images))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Generator\n",
        "\n",
        "We define a custom data generator that yields batches of images and masks. For binary segmentation, we invert the mask (i.e., if the original masks have stones as black and joints as white, we flip that so that stones become white)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def data_generator(image_paths, mask_paths, batch_size, num_classes, input_size=(256,256)):\n",
        "    \"\"\"\n",
        "    Yields batches of (images, masks) for training/validation.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        indices = np.arange(len(image_paths))\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        for start in range(0, len(indices), batch_size):\n",
        "            end = min(start + batch_size, len(indices))\n",
        "            batch_indices = indices[start:end]\n",
        "\n",
        "            images = []\n",
        "            masks = []\n",
        "\n",
        "            for i in batch_indices:\n",
        "                img = load_image(image_paths[i], target_size=input_size, grayscale=False)\n",
        "                mask = load_image(mask_paths[i], target_size=input_size, grayscale=True)\n",
        "\n",
        "                # Scale images and masks to [0,1]\n",
        "                img = img / 255.0\n",
        "                mask = mask / 255.0\n",
        "\n",
        "                # Invert the mask (if original: stones are black, joints are white)\n",
        "                mask = 1.0 - mask\n",
        "\n",
        "                # For binary segmentation, threshold the mask\n",
        "                if num_classes == 2:\n",
        "                    mask = (mask > 0.5).astype(np.float32)  # shape (H, W, 1)\n",
        "                else:\n",
        "                    # For multi-class segmentation, implement one-hot encoding as needed\n",
        "                    pass\n",
        "\n",
        "                images.append(img)\n",
        "                masks.append(mask)\n",
        "\n",
        "            images = np.array(images, dtype=np.float32)\n",
        "            masks = np.array(masks, dtype=np.float32)\n",
        "            yield images, masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Defining the SegFormer Model\n",
        "\n",
        "We now define a simplified SegFormer model. This model uses a combination of convolutional layers to extract features and a few transformer encoder blocks to capture global context. Note that this is a simplified version for demonstration purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def TransformerBlock(x, num_heads, key_dim):\n",
        "    \"\"\"\n",
        "    A simplified transformer encoder block.\n",
        "    \"\"\"\n",
        "    # Multi-head self-attention\n",
        "    attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(x, x)\n",
        "    x = layers.Add()([x, attn_output])\n",
        "    x = layers.LayerNormalization()(x)\n",
        "\n",
        "    # Feed-forward network\n",
        "    ffn = layers.Dense(key_dim * 4, activation='relu')(x)\n",
        "    ffn = layers.Dense(key_dim)(ffn)\n",
        "    x = layers.Add()([x, ffn])\n",
        "    x = layers.LayerNormalization()(x)\n",
        "    return x\n",
        "\n",
        "def SegFormer(input_shape=(256, 256, 3), num_classes=2):\n",
        "    \"\"\"\n",
        "    A simplified SegFormer architecture for semantic segmentation.\n",
        "    \"\"\"\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Backbone: simple convolutional feature extraction\n",
        "    x = layers.Conv2D(64, 3, padding='same', activation='relu')(inputs)\n",
        "    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
        "    # Assume feature map shape is (H, W, 64)\n",
        "\n",
        "    # Flatten spatial dimensions for transformer encoder\n",
        "    shape = tf.shape(x)\n",
        "    H = shape[1]\n",
        "    W = shape[2]\n",
        "    C = x.shape[-1]\n",
        "    x = layers.Reshape((H * W, C))(x)  # shape: (batch, H*W, C)\n",
        "\n",
        "    # Apply a couple of transformer encoder blocks\n",
        "    x = TransformerBlock(x, num_heads=4, key_dim=64)\n",
        "    x = TransformerBlock(x, num_heads=4, key_dim=64)\n",
        "\n",
        "    # Reshape back to spatial dimensions\n",
        "    x = layers.Reshape((H, W, C))(x)\n",
        "\n",
        "    # Decoder: Upsample to original resolution\n",
        "    x = layers.UpSampling2D(size=(4,4), interpolation='bilinear')(x)\n",
        "\n",
        "    # Final classification layer\n",
        "    if num_classes == 2:\n",
        "        outputs = layers.Conv2D(1, 1, activation='sigmoid')(x)\n",
        "    else:\n",
        "        outputs = layers.Conv2D(num_classes, 1, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "# Instantiate the SegFormer model\n",
        "model = SegFormer(input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3), num_classes=NUM_CLASSES)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Compile and Train the Model\n",
        "\n",
        "We now compile the model using the Adam optimizer and the appropriate loss function (binary crossentropy for binary segmentation). We use the `SKIP_TRAINING` flag to load an existing model if available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decide whether to train from scratch or load an existing model\n",
        "SKIP_TRAINING = True  # Set to False to train from scratch\n",
        "\n",
        "if not SKIP_TRAINING:\n",
        "    if NUM_CLASSES == 2:\n",
        "        loss = 'binary_crossentropy'\n",
        "    else:\n",
        "        loss = 'categorical_crossentropy'\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "        loss=loss,\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    train_gen = data_generator(\n",
        "        train_images,\n",
        "        train_masks,\n",
        "        BATCH_SIZE,\n",
        "        NUM_CLASSES,\n",
        "        input_size=(IMAGE_HEIGHT, IMAGE_WIDTH)\n",
        "    )\n",
        "\n",
        "    val_gen = data_generator(\n",
        "        val_images,\n",
        "        val_masks,\n",
        "        BATCH_SIZE,\n",
        "        NUM_CLASSES,\n",
        "        input_size=(IMAGE_HEIGHT, IMAGE_WIDTH)\n",
        "    )\n",
        "\n",
        "    train_steps = len(train_images) // BATCH_SIZE\n",
        "    val_steps = len(val_images) // BATCH_SIZE\n",
        "\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        epochs=EPOCHS,\n",
        "        steps_per_epoch=train_steps,\n",
        "        validation_data=val_gen,\n",
        "        validation_steps=val_steps,\n",
        "        verbose=1\n",
        "    )\n",
        "else:\n",
        "    from tensorflow.keras.models import load_model\n",
        "    model = load_model(\"../models/segformer_chambord.keras\", compile=False)\n",
        "    print(\"Model loaded from disk. Training skipped.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualize Training Curves\n",
        "\n",
        "If training was performed, we can plot the training and validation loss/accuracy curves. Otherwise, a message is printed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "if 'history' in globals():\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Training was skipped, so no training history available for plotting.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Sample Prediction\n",
        "\n",
        "Let's visualize some sample predictions on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_predictions(model, image_paths, mask_paths, input_size=(IMAGE_HEIGHT, IMAGE_WIDTH), num_samples=3):\n",
        "    num_samples = min(num_samples, len(image_paths))\n",
        "    fig, axs = plt.subplots(num_samples, 3, figsize=(12, 4*num_samples))\n",
        "    if num_samples == 1:\n",
        "        axs = np.expand_dims(axs, axis=0)\n",
        "    for i in range(num_samples):\n",
        "        img = load_image(image_paths[i], target_size=input_size, grayscale=False)\n",
        "        mask = load_image(mask_paths[i], target_size=input_size, grayscale=True)\n",
        "        img_scaled = img / 255.0\n",
        "        mask_scaled = mask / 255.0\n",
        "        pred = model.predict(np.expand_dims(img_scaled, axis=0))\n",
        "        if NUM_CLASSES == 2:\n",
        "            pred_mask = (pred[0, :, :, 0] > 0.5).astype(np.uint8)\n",
        "        else:\n",
        "            pred_mask = np.argmax(pred[0], axis=-1)\n",
        "        axs[i, 0].imshow(img.astype(np.uint8))\n",
        "        axs[i, 0].set_title('Image')\n",
        "        axs[i, 0].axis('off')\n",
        "        axs[i, 1].imshow(mask_scaled[:, :, 0], cmap='gray')\n",
        "        axs[i, 1].set_title('Ground Truth Mask')\n",
        "        axs[i, 1].axis('off')\n",
        "        axs[i, 2].imshow(pred_mask, cmap='gray')\n",
        "        axs[i, 2].set_title('Predicted Mask')\n",
        "        axs[i, 2].axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize 5 samples from the validation set\n",
        "visualize_predictions(model, val_images, val_masks, input_size=(IMAGE_HEIGHT, IMAGE_WIDTH), num_samples=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Saving the Model\n",
        "\n",
        "If training was performed, the trained model is saved to disk for later reuse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not SKIP_TRAINING:\n",
        "    SAVE_PATH = \"../models/segformer_chambord.keras\"\n",
        "    model.save(SAVE_PATH)\n",
        "    print(f\"Model saved to {SAVE_PATH}\")\n",
        "else:\n",
        "    print(\"Model not saved because training was skipped.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Post-Training Evaluation: Compute IoU and Dice Metrics\n",
        "\n",
        "We now compute the Intersection over Union (IoU) and Dice Coefficient for the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_iou(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute the Intersection over Union (IoU) for binary masks.\n",
        "    \"\"\"\n",
        "    y_true_f = tf.reshape(y_true, [-1])\n",
        "    y_pred_f = tf.reshape(y_pred, [-1])\n",
        "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
        "    union = tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) - intersection\n",
        "    iou = (intersection + 1e-7) / (union + 1e-7)\n",
        "    return iou.numpy()\n",
        "\n",
        "def compute_dice(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute the Dice Coefficient for binary masks.\n",
        "    \"\"\"\n",
        "    y_true_f = tf.reshape(y_true, [-1])\n",
        "    y_pred_f = tf.reshape(y_pred, [-1])\n",
        "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
        "    dice = (2.0 * intersection + 1e-7) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + 1e-7)\n",
        "    return dice.numpy()\n",
        "\n",
        "iou_scores = []\n",
        "dice_scores = []\n",
        "\n",
        "for img_path, mask_path in zip(val_images, val_masks):\n",
        "    img = load_image(img_path, target_size=(IMAGE_HEIGHT, IMAGE_WIDTH), grayscale=False) / 255.0\n",
        "    mask = load_image(mask_path, target_size=(IMAGE_HEIGHT, IMAGE_WIDTH), grayscale=True) / 255.0\n",
        "    pred = model.predict(np.expand_dims(img, axis=0))\n",
        "    pred_bin = (pred[0, :, :, 0] > 0.5).astype(np.float32)\n",
        "    pred_bin = np.expand_dims(pred_bin, axis=-1)\n",
        "    mask = np.expand_dims(mask[:, :, 0], axis=-1)\n",
        "    current_iou = compute_iou(mask[np.newaxis, ...], pred_bin[np.newaxis, ...])\n",
        "    current_dice = compute_dice(mask[np.newaxis, ...], pred_bin[np.newaxis, ...])\n",
        "    iou_scores.append(current_iou)\n",
        "    dice_scores.append(current_dice)\n",
        "\n",
        "mean_iou = np.mean(iou_scores)\n",
        "mean_dice = np.mean(dice_scores)\n",
        "\n",
        "print(\"Validation Mean IoU: {:.4f}\".format(mean_iou))\n",
        "print(\"Validation Mean Dice: {:.4f}\".format(mean_dice))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Evaluate on External Test Set\n",
        "\n",
        "We now evaluate the model on an external test set. The test images are located in `../test/images` and the expert-provided ground truth masks are in `../test/gt`. Note that the ground truth masks use the inverse convention (stones as black, joints as white), so we invert them before evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TEST_IMAGES_DIR = \"../test/images\"\n",
        "TEST_GT_DIR = \"../test/gt\"\n",
        "\n",
        "test_image_filenames = sorted(os.listdir(TEST_IMAGES_DIR))\n",
        "test_mask_filenames = sorted(os.listdir(TEST_GT_DIR))\n",
        "\n",
        "test_iou_scores = []\n",
        "test_dice_scores = []\n",
        "\n",
        "assert len(test_image_filenames) == len(test_mask_filenames), \"Mismatch between test images and GT masks!\"\n",
        "\n",
        "for img_fname, gt_fname in zip(test_image_filenames, test_mask_filenames):\n",
        "    img_path = os.path.join(TEST_IMAGES_DIR, img_fname)\n",
        "    gt_path = os.path.join(TEST_GT_DIR, gt_fname)\n",
        "    img = load_image(img_path, target_size=(IMAGE_HEIGHT, IMAGE_WIDTH), grayscale=False) / 255.0\n",
        "    gt_mask = load_image(gt_path, target_size=(IMAGE_HEIGHT, IMAGE_WIDTH), grayscale=True) / 255.0\n",
        "    gt_mask_inverted = 1.0 - gt_mask\n",
        "    pred = model.predict(np.expand_dims(img, axis=0))\n",
        "    pred_bin = (pred[0, :, :, 0] > 0.5).astype(np.float32)\n",
        "    pred_bin = np.expand_dims(pred_bin, axis=-1)\n",
        "    current_iou = compute_iou(gt_mask_inverted[np.newaxis, ...], pred_bin[np.newaxis, ...])\n",
        "    current_dice = compute_dice(gt_mask_inverted[np.newaxis, ...], pred_bin[np.newaxis, ...])\n",
        "    test_iou_scores.append(current_iou)\n",
        "    test_dice_scores.append(current_dice)\n",
        "\n",
        "mean_test_iou = np.mean(test_iou_scores)\n",
        "mean_test_dice = np.mean(test_dice_scores)\n",
        "\n",
        "print(f\"Test Mean IoU: {mean_test_iou:.4f}\")\n",
        "print(f\"Test Mean Dice: {mean_test_dice:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Saving Predicted Masks\n",
        "\n",
        "Before moving on, we save the predicted masks for a subset of test images. These masks are saved in a format similar to the ground truth (stones as white, joints as black) and can be included in the final report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "SAVE_MASKS_DIR = \"../predicted_images/segformer_masks\"\n",
        "os.makedirs(SAVE_MASKS_DIR, exist_ok=True)\n",
        "\n",
        "num_samples_to_save = 10\n",
        "\n",
        "for i, (img_fname, gt_fname) in enumerate(zip(test_image_filenames, test_mask_filenames)):\n",
        "    if i >= num_samples_to_save:\n",
        "        break\n",
        "    img_path = os.path.join(TEST_IMAGES_DIR, img_fname)\n",
        "    img = load_image(img_path, target_size=(IMAGE_HEIGHT, IMAGE_WIDTH), grayscale=False) / 255.0\n",
        "    pred = model.predict(np.expand_dims(img, axis=0))\n",
        "    pred_mask = (pred[0, :, :, 0] > 0.5).astype(np.uint8)\n",
        "    pred_mask_255 = (pred_mask * 255).astype(np.uint8)\n",
        "    # Create a 3-channel RGB array to avoid transparency issues\n",
        "    rgb_array = np.stack([pred_mask_255]*3, axis=-1)\n",
        "    mask_img = Image.fromarray(rgb_array, 'RGB')\n",
        "    base_name = os.path.splitext(img_fname)[0]\n",
        "    save_name = f\"pred_{base_name}.jpg\"\n",
        "    save_path = os.path.join(SAVE_MASKS_DIR, save_name)\n",
        "    mask_img.save(save_path, format=\"JPEG\", quality=100)\n",
        "    print(f\"Saved predicted mask for {img_fname} as {save_path}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
