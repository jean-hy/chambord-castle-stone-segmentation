{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fb80c5fd",
      "metadata": {},
      "source": [
        "# DeepLabV3+ for Stone Segmentation\n",
        "\n",
        "This notebook demonstrates how to implement and train a **DeepLabV3+** model for stone segmentation. The workflow is similar to our SegNet implementation and includes:\n",
        "\n",
        "1. Importing dependencies\n",
        "2. Loading and splitting the augmented dataset\n",
        "3. Creating a custom data generator\n",
        "4. Defining the DeepLabV3+ model using a MobileNetV2 backbone\n",
        "5. Compiling and training the model\n",
        "6. Visualizing training curves and sample predictions\n",
        "7. Saving the trained model and predicted masks\n",
        "8. Evaluating model performance using IoU and Dice Coefficient metrics\n",
        "\n",
        "The notebook is saved as `deeplabv3plus_chambord.ipynb` and assumes the dataset is organized as:\n",
        "  - Images: `../data/augmented/images`\n",
        "  - Masks:  `../data/augmented/masks`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6add7ab4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.16.2\n",
            "Keras version: 3.9.0\n",
            "scikit-learn version: 2.16.2\n"
          ]
        }
      ],
      "source": [
        "# Install TensorFlow if needed\n",
        "# !pip install tensorflow\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"Keras version:\", tf.keras.__version__)\n",
        "print(\"scikit-learn version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a175c44",
      "metadata": {},
      "source": [
        "## Hyperparameters and Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0778c037",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define directories for images and masks\n",
        "IMAGES_DIR = \"../data/augmented/images\"\n",
        "MASKS_DIR = \"../data/augmented/masks\"\n",
        "\n",
        "# Image details\n",
        "IMAGE_HEIGHT = 256\n",
        "IMAGE_WIDTH = 256\n",
        "NUM_CLASSES = 2  # Binary segmentation: background and stones\n",
        "\n",
        "# Training parameters\n",
        "BATCH_SIZE = 4\n",
        "EPOCHS = 100\n",
        "VAL_SPLIT = 0.2  # 20% for validation\n",
        "\n",
        "# For reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "156ab441",
      "metadata": {},
      "source": [
        "## 1. Loading and Splitting the Dataset\n",
        "\n",
        "- The `images` folder contains the input images.\n",
        "- The `masks` folder contains corresponding masks (with matching filenames).\n",
        "\n",
        "We load the filenames, build the full paths, and split the data into training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9673f0a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_image(image_path, target_size=(256, 256), grayscale=False):\n",
        "    \"\"\"\n",
        "    Loads an image from disk, resizes it, and converts it to an array.\n",
        "    \"\"\"\n",
        "    img = load_img(image_path, target_size=target_size, color_mode='grayscale' if grayscale else 'rgb')\n",
        "    return img_to_array(img)\n",
        "\n",
        "image_filenames = sorted(os.listdir(IMAGES_DIR))\n",
        "\n",
        "# Build full paths\n",
        "image_paths = [os.path.join(IMAGES_DIR, f) for f in image_filenames]\n",
        "mask_paths = [os.path.join(MASKS_DIR, f) for f in image_filenames]  # assume same filenames\n",
        "\n",
        "print(\"Total images found:\", len(image_paths))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20f1ee56",
      "metadata": {},
      "source": [
        "### Train/Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d88532e",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_images, val_images, train_masks, val_masks = train_test_split(\n",
        "    image_paths, mask_paths, test_size=VAL_SPLIT, random_state=SEED\n",
        ")\n",
        "\n",
        "print(\"Training set size:\", len(train_images))\n",
        "print(\"Validation set size:\", len(val_images))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c8582a3",
      "metadata": {},
      "source": [
        "## 2. Data Generator\n",
        "\n",
        "We define a custom generator that yields batches of images and masks. Note that for binary segmentation, we invert the mask (i.e., if the original masks have stones as black and joints as white, we flip that so that stones become white)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba652e47",
      "metadata": {},
      "outputs": [],
      "source": [
        "def data_generator(image_paths, mask_paths, batch_size, num_classes, input_size=(256,256)):\n",
        "    \"\"\"\n",
        "    Yields batches of (images, masks) for training/validation.\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        indices = np.arange(len(image_paths))\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        for start in range(0, len(indices), batch_size):\n",
        "            end = min(start + batch_size, len(indices))\n",
        "            batch_indices = indices[start:end]\n",
        "\n",
        "            images = []\n",
        "            masks = []\n",
        "\n",
        "            for i in batch_indices:\n",
        "                img = load_image(image_paths[i], target_size=input_size, grayscale=False)\n",
        "                mask = load_image(mask_paths[i], target_size=input_size, grayscale=True)\n",
        "\n",
        "                # Scale to [0,1]\n",
        "                img = img / 255.0\n",
        "                mask = mask / 255.0\n",
        "\n",
        "                # Invert the mask if needed\n",
        "                mask = 1.0 - mask\n",
        "\n",
        "                # For binary segmentation, threshold the mask\n",
        "                if num_classes == 2:\n",
        "                    mask = (mask > 0.5).astype(np.float32)  # shape (H, W, 1)\n",
        "                else:\n",
        "                    # For multi-class, implement one-hot encoding as needed\n",
        "                    pass\n",
        "\n",
        "                images.append(img)\n",
        "                masks.append(mask)\n",
        "\n",
        "            images = np.array(images, dtype=np.float32)\n",
        "            masks = np.array(masks, dtype=np.float32)\n",
        "            yield images, masks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9391bfc5",
      "metadata": {},
      "source": [
        "## 3. Defining the DeepLabV3+ Model\n",
        "\n",
        "We now define a simplified DeepLabV3+ model. In this example, we use a MobileNetV2 backbone pre-trained on ImageNet. We then add an Atrous Spatial Pyramid Pooling (ASPP) module and a decoder. This architecture is widely used for semantic segmentation tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1295a07f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def DeepLabV3Plus(input_shape=(256, 256, 3), num_classes=2):\n",
        "    \"\"\"\n",
        "    A simplified DeepLabV3+ architecture using MobileNetV2 as the backbone.\n",
        "    \"\"\"\n",
        "    base_model = tf.keras.applications.MobileNetV2(\n",
        "        input_shape=input_shape,\n",
        "        include_top=False,\n",
        "        weights='imagenet'\n",
        "    )\n",
        "\n",
        "    # Use 'block_13_expand_relu' as the low resolution feature map\n",
        "    x = base_model.get_layer('block_13_expand_relu').output\n",
        "\n",
        "    # Atrous Spatial Pyramid Pooling (ASPP) module\n",
        "    # A simple ASPP with a 1x1 conv and two 3x3 conv layers with different dilation rates\n",
        "    aspp1 = layers.Conv2D(256, (1,1), padding='same', use_bias=False)(x)\n",
        "    aspp1 = layers.BatchNormalization()(aspp1)\n",
        "    aspp1 = layers.Activation('relu')(aspp1)\n",
        "\n",
        "    aspp2 = layers.Conv2D(256, (3,3), dilation_rate=6, padding='same', use_bias=False)(x)\n",
        "    aspp2 = layers.BatchNormalization()(aspp2)\n",
        "    aspp2 = layers.Activation('relu')(aspp2)\n",
        "\n",
        "    aspp3 = layers.Conv2D(256, (3,3), dilation_rate=12, padding='same', use_bias=False)(x)\n",
        "    aspp3 = layers.BatchNormalization()(aspp3)\n",
        "    aspp3 = layers.Activation('relu')(aspp3)\n",
        "\n",
        "    # Concatenate ASPP features\n",
        "    x = layers.Concatenate()([aspp1, aspp2, aspp3])\n",
        "\n",
        "    # Decoder: Upsample and combine with low-level features\n",
        "    x = layers.UpSampling2D(size=(4,4), interpolation='bilinear')(x)\n",
        "\n",
        "    # Optionally\n",
        "    low_level_feat = base_model.get_layer('block_3_expand_relu').output\n",
        "    low_level_feat = layers.Conv2D(48, (1,1), padding='same', use_bias=False)(low_level_feat)\n",
        "    low_level_feat = layers.BatchNormalization()(low_level_feat)\n",
        "    low_level_feat = layers.Activation('relu')(low_level_feat)\n",
        "\n",
        "    x = layers.Concatenate()([x, low_level_feat])\n",
        "    x = layers.Conv2D(256, (3,3), padding='same', use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    # Final upsampling\n",
        "    x = layers.UpSampling2D(size=(4,4), interpolation='bilinear')(x)\n",
        "\n",
        "    # Final classification layer\n",
        "    if num_classes == 2:\n",
        "        # For binary segmentation\n",
        "        x = layers.Conv2D(1, (1,1), padding='same', activation='sigmoid')(x)\n",
        "    else:\n",
        "        # For multi-class segmentation\n",
        "        x = layers.Conv2D(num_classes, (1,1), padding='same', activation='softmax')(x)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=base_model.input, outputs=x)\n",
        "    return model\n",
        "\n",
        "# Instantiate the DeepLabV3+ model\n",
        "model = DeepLabV3Plus(input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3), num_classes=NUM_CLASSES)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65b87530",
      "metadata": {},
      "source": [
        "## 4. Compile and Train the Model\n",
        "\n",
        "We now compile the model using the Adam optimizer and the appropriate loss function (binary crossentropy for 2 classes). We use a flag (`SKIP_TRAINING`) to optionally load the saved model instead of retraining."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d74d49a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Decide whether to train from scratch or load an existing model\n",
        "SKIP_TRAINING = True  # Set to False to train from scratch\n",
        "\n",
        "if not SKIP_TRAINING:\n",
        "    if NUM_CLASSES == 2:\n",
        "        loss = 'binary_crossentropy'\n",
        "    else:\n",
        "        loss = 'categorical_crossentropy'\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "        loss=loss,\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    # Create generators\n",
        "    train_gen = data_generator(\n",
        "        train_images,\n",
        "        train_masks,\n",
        "        BATCH_SIZE,\n",
        "        NUM_CLASSES,\n",
        "        input_size=(IMAGE_HEIGHT, IMAGE_WIDTH)\n",
        "    )\n",
        "\n",
        "    val_gen = data_generator(\n",
        "        val_images,\n",
        "        val_masks,\n",
        "        BATCH_SIZE,\n",
        "        NUM_CLASSES,\n",
        "        input_size=(IMAGE_HEIGHT, IMAGE_WIDTH)\n",
        "    )\n",
        "\n",
        "    train_steps = len(train_images) // BATCH_SIZE\n",
        "    val_steps = len(val_images) // BATCH_SIZE\n",
        "\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        epochs=EPOCHS,\n",
        "        steps_per_epoch=train_steps,\n",
        "        validation_data=val_gen,\n",
        "        validation_steps=val_steps,\n",
        "        verbose=1\n",
        "    )\n",
        "else:\n",
        "    from tensorflow.keras.models import load_model\n",
        "    model = load_model(\"../models/deeplabv3plus_chambord.keras\", compile=False)\n",
        "    print(\"Model loaded from disk. Training skipped.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daa75f2c",
      "metadata": {},
      "source": [
        "## 5. Visualize Training Curves\n",
        "\n",
        "If training was performed, we can visualize the loss and accuracy curves. If training was skipped, a message will be printed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f97bde91",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "if 'history' in globals():\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Training was skipped, so no training history available for plotting.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7133c87a",
      "metadata": {},
      "source": [
        "## 6. Sample Prediction\n",
        "\n",
        "Let's visualize some sample predictions on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ebc0e25",
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_predictions(model, image_paths, mask_paths, input_size=(IMAGE_HEIGHT, IMAGE_WIDTH), num_samples=3):\n",
        "    num_samples = min(num_samples, len(image_paths))\n",
        "    fig, axs = plt.subplots(num_samples, 3, figsize=(12, 4*num_samples))\n",
        "    if num_samples == 1:\n",
        "        axs = np.expand_dims(axs, axis=0)\n",
        "    for i in range(num_samples):\n",
        "        img = load_image(image_paths[i], target_size=input_size, grayscale=False)\n",
        "        mask = load_image(mask_paths[i], target_size=input_size, grayscale=True)\n",
        "        img_scaled = img / 255.0\n",
        "        mask_scaled = mask / 255.0\n",
        "        pred = model.predict(np.expand_dims(img_scaled, axis=0))\n",
        "        if NUM_CLASSES == 2:\n",
        "            pred_mask = (pred[0, :, :, 0] > 0.5).astype(np.uint8)\n",
        "        else:\n",
        "            pred_mask = np.argmax(pred[0], axis=-1)\n",
        "        axs[i, 0].imshow(img.astype(np.uint8))\n",
        "        axs[i, 0].set_title('Image')\n",
        "        axs[i, 0].axis('off')\n",
        "        axs[i, 1].imshow(mask_scaled[:, :, 0], cmap='gray')\n",
        "        axs[i, 1].set_title('Ground Truth Mask')\n",
        "        axs[i, 1].axis('off')\n",
        "        axs[i, 2].imshow(pred_mask, cmap='gray')\n",
        "        axs[i, 2].set_title('Predicted Mask')\n",
        "        axs[i, 2].axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize 5 samples from the validation set\n",
        "visualize_predictions(model, val_images, val_masks, input_size=(IMAGE_HEIGHT, IMAGE_WIDTH), num_samples=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e04cddb8",
      "metadata": {},
      "source": [
        "## 7. Saving the Model\n",
        "\n",
        "If training was performed, the trained model is saved to disk for later reuse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb830ecf",
      "metadata": {},
      "outputs": [],
      "source": [
        "if not SKIP_TRAINING:\n",
        "    SAVE_PATH = \"../models/deeplabv3plus_chambord.keras\"\n",
        "    model.save(SAVE_PATH)\n",
        "    print(f\"Model saved to {SAVE_PATH}\")\n",
        "else:\n",
        "    print(\"Model not saved because training was skipped.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fecf18a",
      "metadata": {},
      "source": [
        "## 8. Post-Training Evaluation: Compute IoU and Dice Metrics\n",
        "\n",
        "We now compute the Intersection over Union (IoU) and Dice Coefficient for the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fe18686",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_iou(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute the Intersection over Union (IoU) for binary masks.\n",
        "    \"\"\"\n",
        "    y_true_f = tf.reshape(y_true, [-1])\n",
        "    y_pred_f = tf.reshape(y_pred, [-1])\n",
        "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
        "    union = tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) - intersection\n",
        "    iou = (intersection + 1e-7) / (union + 1e-7)\n",
        "    return iou.numpy()\n",
        "\n",
        "def compute_dice(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute the Dice Coefficient for binary masks.\n",
        "    \"\"\"\n",
        "    y_true_f = tf.reshape(y_true, [-1])\n",
        "    y_pred_f = tf.reshape(y_pred, [-1])\n",
        "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
        "    dice = (2.0 * intersection + 1e-7) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + 1e-7)\n",
        "    return dice.numpy()\n",
        "\n",
        "iou_scores = []\n",
        "dice_scores = []\n",
        "\n",
        "for img_path, mask_path in zip(val_images, val_masks):\n",
        "    img = load_image(img_path, target_size=(IMAGE_HEIGHT, IMAGE_WIDTH), grayscale=False) / 255.0\n",
        "    mask = load_image(mask_path, target_size=(IMAGE_HEIGHT, IMAGE_WIDTH), grayscale=True) / 255.0\n",
        "    pred = model.predict(np.expand_dims(img, axis=0))\n",
        "    pred_bin = (pred[0, :, :, 0] > 0.5).astype(np.float32)\n",
        "    pred_bin = np.expand_dims(pred_bin, axis=-1)\n",
        "    mask = np.expand_dims(mask[:, :, 0], axis=-1)\n",
        "    current_iou = compute_iou(mask[np.newaxis, ...], pred_bin[np.newaxis, ...])\n",
        "    current_dice = compute_dice(mask[np.newaxis, ...], pred_bin[np.newaxis, ...])\n",
        "    iou_scores.append(current_iou)\n",
        "    dice_scores.append(current_dice)\n",
        "\n",
        "mean_iou = np.mean(iou_scores)\n",
        "mean_dice = np.mean(dice_scores)\n",
        "\n",
        "print(\"Validation Mean IoU: {:.4f}\".format(mean_iou))\n",
        "print(\"Validation Mean Dice: {:.4f}\".format(mean_dice))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3465bb7",
      "metadata": {},
      "source": [
        "## 9. Evaluate on External Test Set\n",
        "\n",
        "We now evaluate the model on an external test set. The test images are located in `../test/images` and the expert-provided ground truth masks are in `../test/gt`. Note that the ground truth masks use the inverse convention (stones as black, joints as white), so we invert them before evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fea6079a",
      "metadata": {},
      "outputs": [],
      "source": [
        "TEST_IMAGES_DIR = \"../test/images\"\n",
        "TEST_GT_DIR = \"../test/gt\"\n",
        "\n",
        "test_image_filenames = sorted(os.listdir(TEST_IMAGES_DIR))\n",
        "test_mask_filenames = sorted(os.listdir(TEST_GT_DIR))\n",
        "\n",
        "test_iou_scores = []\n",
        "test_dice_scores = []\n",
        "\n",
        "assert len(test_image_filenames) == len(test_mask_filenames), \"Mismatch between test images and GT masks!\"\n",
        "\n",
        "for img_fname, gt_fname in zip(test_image_filenames, test_mask_filenames):\n",
        "    img_path = os.path.join(TEST_IMAGES_DIR, img_fname)\n",
        "    gt_path = os.path.join(TEST_GT_DIR, gt_fname)\n",
        "    img = load_image(img_path, target_size=(IMAGE_HEIGHT, IMAGE_WIDTH), grayscale=False) / 255.0\n",
        "    gt_mask = load_image(gt_path, target_size=(IMAGE_HEIGHT, IMAGE_WIDTH), grayscale=True) / 255.0\n",
        "    gt_mask_inverted = 1.0 - gt_mask\n",
        "    pred = model.predict(np.expand_dims(img, axis=0))\n",
        "    pred_bin = (pred[0, :, :, 0] > 0.5).astype(np.float32)\n",
        "    pred_bin = np.expand_dims(pred_bin, axis=-1)\n",
        "    current_iou = compute_iou(gt_mask_inverted[np.newaxis, ...], pred_bin[np.newaxis, ...])\n",
        "    current_dice = compute_dice(gt_mask_inverted[np.newaxis, ...], pred_bin[np.newaxis, ...])\n",
        "    test_iou_scores.append(current_iou)\n",
        "    test_dice_scores.append(current_dice)\n",
        "\n",
        "mean_test_iou = np.mean(test_iou_scores)\n",
        "mean_test_dice = np.mean(test_dice_scores)\n",
        "\n",
        "print(f\"Test Mean IoU: {mean_test_iou:.4f}\")\n",
        "print(f\"Test Mean Dice: {mean_test_dice:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97da65c1",
      "metadata": {},
      "source": [
        "## 10. Saving Predicted Masks\n",
        "\n",
        "Before moving on, we save the predicted masks for a subset of test images. These masks are saved in a format similar to the ground truth (stones as white, joints as black) and can be included in the final report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ac2c886",
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "SAVE_MASKS_DIR = \"../predicted_images/deeplabv3plus_masks\"\n",
        "os.makedirs(SAVE_MASKS_DIR, exist_ok=True)\n",
        "\n",
        "num_samples_to_save = 10\n",
        "\n",
        "for i, (img_fname, gt_fname) in enumerate(zip(test_image_filenames, test_mask_filenames)):\n",
        "    if i >= num_samples_to_save:\n",
        "        break\n",
        "    img_path = os.path.join(TEST_IMAGES_DIR, img_fname)\n",
        "    img = load_image(img_path, target_size=(IMAGE_HEIGHT, IMAGE_WIDTH), grayscale=False) / 255.0\n",
        "    pred = model.predict(np.expand_dims(img, axis=0))\n",
        "    pred_mask = (pred[0, :, :, 0] > 0.5).astype(np.uint8)\n",
        "    pred_mask_255 = (pred_mask * 255).astype(np.uint8)\n",
        "    # Create a 3-channel RGB array to avoid transparency issues\n",
        "    rgb_array = np.stack([pred_mask_255]*3, axis=-1)\n",
        "    mask_img = Image.fromarray(rgb_array, 'RGB')\n",
        "    base_name = os.path.splitext(img_fname)[0]\n",
        "    save_name = f\"pred_{base_name}.jpg\"\n",
        "    save_path = os.path.join(SAVE_MASKS_DIR, save_name)\n",
        "    mask_img.save(save_path, format=\"JPEG\", quality=100)\n",
        "    print(f\"Saved predicted mask for {img_fname} as {save_path}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
